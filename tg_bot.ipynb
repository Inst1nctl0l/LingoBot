{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install h5py\n",
        "%pip install typing-extensions\n",
        "%pip install wheel\n",
        "%pip install transformers nest-asyncio torch\n",
        "%pip install aiogram==2.25.1\n",
        "%pip install ollama\n",
        "%pip install langchain\n",
        "%pip install babel==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwxNAHpTGuWV"
      },
      "outputs": [],
      "source": [
        "from aiogram import Bot, Dispatcher, executor, types\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import AutoTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "import torch\n",
        "import re\n",
        "import nest_asyncio\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "import torch\n",
        "import ollama\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "import time\n",
        "from cachetools import TTLCache\n",
        "import requests\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.contrib.middlewares.logging import LoggingMiddleware\n",
        "from aiogram.contrib.fsm_storage.memory import MemoryStorage\n",
        "from aiogram.dispatcher import FSMContext\n",
        "from aiogram.dispatcher.filters.state import State, StatesGroup\n",
        "from aiogram.utils import executor\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5zYZS8A30ocN",
        "outputId": "3a8ab200-38e8-4684-c84e-ec969cbda34c"
      },
      "outputs": [],
      "source": [
        "# !ollama pull llama3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !curl https://ollama.ai/install.sh | sh \n",
        "# /home/instinctlol/DL_project/install.sh - в терминале!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl -X POST http://localhost:11434/api/pull -d '{\"model\":\"llama2-uncensored\" }'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !curl -X POST http://localhost:11434/api/pull -d '{\"model\":\"llama3\" }'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ollama pull llama2-uncensored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNxMc9XeuzdQ",
        "outputId": "be2f6252-7a33-4473-d5d3-cf2e70b95d96"
      },
      "outputs": [],
      "source": [
        "\n",
        "response = ollama.chat(model='llama2-uncensored', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': 'Who is Moai?',\n",
        "  },\n",
        "])\n",
        "print(response['message']['content'])\n",
        "# little check if the model works just fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJo1Tx-qQZzF",
        "outputId": "27ca27e4-f5e9-4fa2-cd6b-018d4dfa267d"
      },
      "outputs": [],
      "source": [
        "# import nest_asyncio\n",
        "# from aiogram import Bot, Dispatcher, executor, types\n",
        "# import ollama\n",
        "# import logging\n",
        "\n",
        "# # Инициализация бота\n",
        "# API_TOKEN = '7135255841:AAFZ0pVLhf7YI_wq6qZdH-VFXQZWyD_R9ec'\n",
        "# bot = Bot(token=API_TOKEN)\n",
        "# dp = Dispatcher(bot)\n",
        "\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# @dp.message_handler()\n",
        "# async def handle_message(message: types.Message):\n",
        "#     user_input = message.text \n",
        "\n",
        "#     try:\n",
        "#         # Запрос к модели\n",
        "#         response = ollama.chat(model='llama2-uncensored', messages=[ #llama3\n",
        "#             {\n",
        "#                 'role': 'user',\n",
        "#                 'content': user_input,\n",
        "#             },\n",
        "#         ])\n",
        "#         # if user_input == ''\n",
        "#         quote = response['message']['content']\n",
        "#     except Exception as e:\n",
        "#         logging.error(f\"Ошибка при запросе к модели: {e}\")\n",
        "#         quote = \"Произошла ошибка при обработке вашего запроса. Пожалуйста, попробуйте позже.\"\n",
        "\n",
        "#     # Отправка сгенерированного ответа\n",
        "#     await message.reply(quote)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     nest_asyncio.apply()  # Разрешение вложенных циклов asyncio\n",
        "#     executor.start_polling(dp, skip_updates=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install cachetools\n",
        "%pip install googletrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9ZKr0tAlQmh"
      },
      "outputs": [],
      "source": [
        "# Инициализация бота\n",
        "API_TOKEN = '7135255841:AAFZ0pVLhf7YI_wq6qZdH-VFXQZWyD_R9ec'\n",
        "bot = Bot(token=API_TOKEN)\n",
        "dp = Dispatcher(bot)\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Кэширование ответов\n",
        "cache = TTLCache(maxsize=100, ttl=600)\n",
        "\n",
        "# Ограничение количества запросов\n",
        "user_requests = defaultdict(list)\n",
        "REQUEST_LIMIT = 5\n",
        "TIME_WINDOW = 60  # Время в секундах\n",
        "\n",
        "# Список администраторов (исключения по количеству запросов)\n",
        "ADMIN_USERS = [399989639]  # Замените на ваши ID\n",
        "\n",
        "# Словарь для хранения истории сообщений\n",
        "user_message_history = defaultdict(list)\n",
        "\n",
        "@dp.message_handler(commands=['weather'])\n",
        "async def weather(message: types.Message):\n",
        "    await message.reply(\"Введи свой город, дорогой\")\n",
        "    await WeatherStates.waiting_for_city.set()\n",
        "\n",
        "class WeatherStates(StatesGroup):\n",
        "    waiting_for_city = State()\n",
        "\n",
        "@dp.message_handler(commands=['weather'])\n",
        "async def weather(message: types.Message):\n",
        "    await message.reply(\"Введи свой город, дорогой\")\n",
        "    await WeatherStates.waiting_for_city.set()\n",
        "\n",
        "@dp.message_handler(state=WeatherStates.waiting_for_city)\n",
        "async def handle_message(message: types.Message, state: FSMContext):\n",
        "    city = message.text.lower()\n",
        "    await message.reply(\"Ваш город: \" + city)\n",
        "\n",
        "    # Формируем запрос\n",
        "    url = f'https://api.openweathermap.org/data/2.5/weather?q={city}&units=metric&lang=ru&appid=79d1ca96933b0328e1c7e3e7a26cb347'\n",
        "    try:\n",
        "            # Отправляем запрос на сервер и сразу получаем результат\n",
        "            weather_data = requests.get(url).json()\n",
        "\n",
        "            if weather_data.get('cod') != 200:\n",
        "                await message.reply(\"Не удалось найти погоду для указанного города. Пожалуйста, попробуйте снова.\")\n",
        "            else:\n",
        "                # Получаем данные о температуре и о том, как она ощущается\n",
        "                temperature = round(weather_data['main']['temp'])\n",
        "                temperature_feels = round(weather_data['main']['feels_like'])\n",
        "\n",
        "                # Выводим значения на экран\n",
        "                now = f'Сейчас в городе {city} {temperature} °C\\n'\n",
        "                feels_like = f'Ощущается как {temperature_feels} °C'\n",
        "                await message.reply(now + feels_like)\n",
        "    except Exception as e:\n",
        "            await message.reply(f\"Произошла ошибка: {e}\")\n",
        "    # Завершаем состояние\n",
        "    await state.finish()\n",
        "\n",
        "@dp.message_handler(commands=['help'])\n",
        "async def send_help(message: types.Message):\n",
        "    help_text = (\"Я бот, который использует модель Llama для ответов на ваши вопросы. \"\n",
        "                 \"Просто отправьте мне сообщение, и я постараюсь вам помочь!\\n\"\n",
        "                 \"Доступные команды:\\n\"\n",
        "                 \"/help - Показать это сообщение.\")\n",
        "    await message.reply(help_text)\n",
        "\n",
        "@dp.message_handler()\n",
        "async def handle_message(message: types.Message):\n",
        "    user_input = message.text.lower()  # Приведение запроса к нижнему регистру\n",
        "    user_id = message.from_user.id\n",
        "    username = message.from_user.username\n",
        "    current_time = time.time()\n",
        "\n",
        "    logging.info(f\"Received message from {username} (ID: {user_id}): {user_input}\")\n",
        "\n",
        "    # Проверка на администратора\n",
        "    if user_id not in ADMIN_USERS:\n",
        "        # Очистка старых запросов\n",
        "        user_requests[user_id] = [req for req in user_requests[user_id] if current_time - req < TIME_WINDOW]\n",
        "\n",
        "        if len(user_requests[user_id]) >= REQUEST_LIMIT:\n",
        "            await message.reply(\"Вы превысили лимит запросов. Пожалуйста, подождите некоторое время перед отправкой нового запроса.\")\n",
        "            return\n",
        "\n",
        "        user_requests[user_id].append(current_time)\n",
        "\n",
        "    # Получаем историю сообщений пользователя\n",
        "    user_history = user_message_history[user_id]\n",
        "\n",
        "    # Добавляем новое сообщение пользователя в историю\n",
        "    user_history.append({\n",
        "        'role': 'user',\n",
        "        'content': user_input,\n",
        "    })\n",
        "\n",
        "    # Ограничиваем историю последними N сообщениями\n",
        "    MAX_HISTORY = 5  # Максимальное количество сообщений в истории\n",
        "    user_history = user_history[-MAX_HISTORY:]\n",
        "\n",
        "    # Проверка, есть ли ответ в кэше\n",
        "    if user_input in cache:\n",
        "        quote = cache[user_input]\n",
        "    else:\n",
        "        try:\n",
        "            # Запрос к модели Llama с учетом истории\n",
        "            response = ollama.chat(model='llama2-uncensored', messages=user_history)\n",
        "            quote = response['message']['content']\n",
        "\n",
        "            # Сохранение ответа в кэше\n",
        "            cache[user_input] = quote\n",
        "            user_message_history[user_id] = user_history\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error while querying the model: {e}\")\n",
        "            quote = \"Произошла ошибка при обработке вашего запроса. Пожалуйста, попробуйте позже.\"\n",
        "\n",
        "    logging.info(f\"Replying to {username} (ID: {user_id}): {quote}\")\n",
        "    await message.reply(quote)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    nest_asyncio.apply()  # Разрешение вложенных циклов asyncio\n",
        "    executor.start_polling(dp, skip_updates=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 -m pip install -U git+https://github.com/facebookresearch/audiocraft#egg=audiocraft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 -m pip install -U audiocraft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from audiocraft.models import musicgen\n",
        "from audiocraft.utils.notebook import display_audio\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = musicgen.MusicGen.get_pretrained('medium', device='cuda')\n",
        "model.set_generation_params(duration=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = model.generate([\n",
        "    'crazy EDM, heavy bang', \n",
        "    'classic reggae track with an electronic guitar solo',\n",
        "    'lofi slow bpm electro chill with organic samples',\n",
        "    'rock with saturated guitars, a heavy bass line and crazy drum break and fills.',\n",
        "    'earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy, easygoing, organic instrumentation, gentle grooves',\n",
        "], \n",
        "    progress=True)\n",
        "display_audio(res, 32000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dp.message_handler(commands=['music'])\n",
        "async def send_help(message: types.Message):\n",
        "    help_text = (\"Я бот, который использует модель Llama для ответов на ваши вопросы. \"\n",
        "                 \"Просто отправьте мне сообщение, и я постараюсь вам помочь!\\n\"\n",
        "                 \"Доступные команды:\\n\"\n",
        "                 \"/help - Показать это сообщение.\")\n",
        "    await message.reply(help_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d2964f1ce5d4596bcb3b0ebea8b86ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb09b34ffd29455d8c4b97d55a5aaf67",
              "IPY_MODEL_39fa390ebac74e37ac800164e30a46e8",
              "IPY_MODEL_e7dc3651a7c04a418d804f739a6ec1ae"
            ],
            "layout": "IPY_MODEL_1177f53969b345f4be9f749522c9f16e"
          }
        },
        "1177f53969b345f4be9f749522c9f16e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1711c2aa75104d429916973777e9c6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3450375586a240b58756d1eee0d6e114": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fa390ebac74e37ac800164e30a46e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d70282d6fd4427b742c834441e91d8",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1711c2aa75104d429916973777e9c6ea",
            "value": 0
          }
        },
        "53acde4452cd4ea0914a2768af442d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987d5c9a0e5143fd89cc12827f91e563": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bb536aadfcf4c21be8bc1c977b3b80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb09b34ffd29455d8c4b97d55a5aaf67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53acde4452cd4ea0914a2768af442d8e",
            "placeholder": "​",
            "style": "IPY_MODEL_9bb536aadfcf4c21be8bc1c977b3b80d",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "e7dc3651a7c04a418d804f739a6ec1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3450375586a240b58756d1eee0d6e114",
            "placeholder": "​",
            "style": "IPY_MODEL_987d5c9a0e5143fd89cc12827f91e563",
            "value": " 0/6 [00:00&lt;?, ?it/s]"
          }
        },
        "f1d70282d6fd4427b742c834441e91d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
